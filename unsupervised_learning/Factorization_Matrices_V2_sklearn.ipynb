{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, DataFrame, HiveContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "sqlContext_H = HiveContext(sc)\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "# Read log files and calculate TF-IDF\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fields_list = (\n",
    "    \"ACCESS_DTTM\", \"ACCESS_MONTH\", \"ACCESS_WEEK\", \"ACCESS_DAY\", \"ACCESS_HOUR\", \"USER_ID\", \"WORKSTATION\"\n",
    ")\n",
    "full_data = sqlContext_H.read.parquet(HOST + \"/parquet2/*/*\").select(*fields_list).persist()\n",
    "sqlContext_H.registerDataFrameAsTable(full_data, 'full_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total_users = sc.textFile(HOST + \"/csv/unique_users.csv\").count()\n",
    "total_workstations = sc.textFile(HOST + \"/csv/unique_workstations.csv\").count()\n",
    "print \"Total users:\", total_users\n",
    "print \"Total workstations:\", total_workstations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "query = lambda table, group_by, total_users: \" \".join([\n",
    "    \"SELECT mainT1.USER_ID, mainT1.WORKSTATION, {}, mainT1.TF * mainT2.IDF AS TF_IDF\".format(\", \".join([\"mainT1.\"+i for i in group_by.replace(\" \",\"\").split(\",\")])),\n",
    "    \"FROM (SELECT T1.USER_ID, T1.WORKSTATION, {}, T1.AMOUNT*1.0 / T2.TOTAL AS TF\".format(\", \".join([\"T1.\"+i for i in group_by.replace(\" \",\"\").split(\",\")])),   \n",
    "        \"FROM (SELECT USER_ID, {group_by}, WORKSTATION, COUNT(ACCESS_DTTM) AS AMOUNT\",\n",
    "            \"FROM {table_name}\",\n",
    "            \"GROUP BY USER_ID, {group_by}, WORKSTATION\",\n",
    "        \") AS T1\",\n",
    "        \"JOIN (SELECT USER_ID, {group_by}, COUNT(ACCESS_DTTM) AS TOTAL\",\n",
    "            \"FROM {table_name}\",\n",
    "            \"GROUP BY USER_ID, {group_by}\",\n",
    "        \") AS T2\",\n",
    "        \"ON T1.USER_ID = T2.USER_ID AND {}\".format(\" AND \".join([\"T1.\"+i+\" = T2.\"+i for i in group_by.replace(\" \",\"\").split(\",\")])),\n",
    "    \") AS mainT1 JOIN (\",\n",
    "        \"SELECT USER_ID, {group_by}, log10(1 + {total_users}*1.0 / COUNT(DISTINCT(WORKSTATION))) AS IDF\",\n",
    "        \"FROM {table_name}\",\n",
    "        \"GROUP BY USER_ID, {group_by}\",\n",
    "    \") AS mainT2 ON mainT1.USER_ID = mainT2.USER_ID AND {}\".format(\" AND \".join([\"mainT1.\"+i+\" = mainT2.\"+i for i in group_by.replace(\" \",\"\").split(\",\")])),\n",
    "    ]).format(table_name=table, group_by=group_by, total_users=total_users)\n",
    "\n",
    "res = sqlContext_H.sql(query(\"full_data\", \"ACCESS_WEEK\", total_users)).persist()\n",
    "res.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(16, 7))\n",
    "x = np.array(map(lambda x: x[0], res.select(\"TF_IDF\").collect()))\n",
    "plt.hist(x, 100, facecolor='green', alpha=0.75)\n",
    "plt.xlabel(\"TF_IDF\", fontsize=12)\n",
    "plt.ylabel('Amount', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Factorization\n",
    "\n",
    "### Read data of the 7th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data.unpersist()\n",
    "    all_users.unpersist()\n",
    "    all_workstations.unpersist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "month = 7\n",
    "fields_list = ( \"ACCESS_DTTM\", \"USER_ID\", \"WORKSTATION\" )\n",
    "data = sqlContext_H.read.parquet( HOST + \"/parquet2/0{}/*\".format(month) ).select(*fields_list).persist()\n",
    "sqlContext_H.registerDataFrameAsTable(data, 'data')\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate TF-IDF and encode USER_ID and WORKSTATION to label indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Count all unique users of the defined month\n",
    "overwrite = False\n",
    "if overwrite:\n",
    "    all_users = sqlContext_H.sql(\"SELECT DISTINCT(USER_ID) AS count FROM data\")    \n",
    "    all_users.write.format(\"com.databricks.spark.csv\").mode('overwrite').save(HOST + \"/csv/unique_users_{}.csv\".format(month))\n",
    "\n",
    "total_users = sc.textFile(HOST + \"/csv/unique_users_{}.csv\".format(month)).count()\n",
    "\n",
    "# 2. Calculate tf-idf\n",
    "query = lambda table, total_users: \"\"\"\n",
    "    SELECT mainT1.USER_ID, mainT1.WORKSTATION, mainT1.TF * mainT2.IDF AS TF_IDF, mainT2.IDF\n",
    "    FROM (SELECT T1.USER_ID, T1.WORKSTATION, T1.AMOUNT*1.0 / T2.TOTAL AS TF \n",
    "        FROM (SELECT USER_ID, WORKSTATION, COUNT(ACCESS_DTTM) AS AMOUNT\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID, WORKSTATION\n",
    "        ) AS T1\n",
    "        JOIN (SELECT USER_ID, COUNT(ACCESS_DTTM) AS TOTAL\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID\n",
    "        ) AS T2\n",
    "        ON T1.USER_ID = T2.USER_ID\n",
    "    ) AS mainT1 JOIN (\n",
    "        SELECT T.USER_ID, log10( 1 + {total_users} * 1.0 / COUNT(T.WORKSTATION) ) AS IDF\n",
    "        FROM (SELECT USER_ID, WORKSTATION\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID, WORKSTATION\n",
    "        ) AS T\n",
    "        GROUP BY T.USER_ID\n",
    "    ) AS mainT2 ON mainT1.USER_ID = mainT2.USER_ID\n",
    "\"\"\".format(table_name=table, total_users=total_users)\n",
    "\n",
    "res = sqlContext_H.sql(query(\"data\", total_users)).persist()\n",
    "\n",
    "# 3. Encode a string column of labels to a column of label indices\n",
    "all_users = sqlContext_H.createDataFrame( \n",
    "        sc.textFile(HOST + \"/csv/unique_users.csv\").map(lambda p: Row(USER_ID=p)) \n",
    "    ).union( sqlContext_H.createDataFrame([Row(USER_ID='')]) ).persist()\n",
    "all_workstations = sqlContext_H.createDataFrame( \n",
    "        sc.textFile(HOST + \"/csv/unique_workstations.csv\").map(lambda p: Row(WORKSTATION=p)) \n",
    "    ).union( sqlContext_H.createDataFrame([Row(WORKSTATION='')]) ).persist()\n",
    "\n",
    "indexerU = StringIndexer(inputCol=\"USER_ID\", outputCol=\"USER_ID_Index\").fit(all_users)\n",
    "indexerW = StringIndexer(inputCol=\"WORKSTATION\", outputCol=\"WORKSTATION_Index\").fit(all_workstations)\n",
    "\n",
    "all_users.unpersist()\n",
    "all_workstations.unpersist()\n",
    "\n",
    "indexedU_df = indexerU.transform(res).withColumn(\n",
    "        \"USER_ID_Index\", F.col(\"USER_ID_Index\").cast(IntegerType())\n",
    "    ).persist()\n",
    "\n",
    "table = indexerW.transform(indexedU_df).withColumn(\n",
    "        \"WORKSTATION_Index\", F.col(\"WORKSTATION_Index\").cast(IntegerType())\n",
    "    ).persist()\n",
    "\n",
    "res.unpersist()\n",
    "indexedU_df.unpersist()\n",
    "table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "t = table.select(\"USER_ID_Index\", \"WORKSTATION_Index\", \"TF_IDF\").toPandas()\n",
    "\n",
    "total_users = sc.textFile(HOST + \"/csv/unique_users.csv\").count()\n",
    "total_workstations = sc.textFile(HOST + \"/csv/unique_workstations.csv\").count()\n",
    "\n",
    "def sparse_df_to_array(df, shape):\n",
    "    \"\"\" Convert sparse dataframe to sparse array csr_matrix used by scikit learn. \"\"\"\n",
    "    arr = lil_matrix(shape, dtype=np.float32)\n",
    "    for i in range(df.shape[0]):\n",
    "        arr[df.ix[i, \"USER_ID_Index\"]-1, df.ix[i, \"WORKSTATION_Index\"]-1] = df.ix[i, \"TF_IDF\"]\n",
    "    return arr.tocsr()\n",
    "\n",
    "m = sparse_df_to_array(t, (total_users, total_workstations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "svd = TruncatedSVD(n_components=10, n_iter=15, random_state=42)\n",
    "svd.fit(m) \n",
    "print(svd.explained_variance_ratio_) \n",
    "print(svd.explained_variance_ratio_.sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Caclulate cosine similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Prepare dataframes for each day from the 8th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "query = lambda table: \"\"\"\n",
    "    SELECT mainT1.USER_ID, mainT1.WORKSTATION, mainT1.TF * mainT2.IDF AS TF_IDF\n",
    "    FROM (SELECT T1.USER_ID, T1.WORKSTATION, T1.AMOUNT*1.0 / T2.TOTAL AS TF \n",
    "        FROM (SELECT USER_ID, WORKSTATION, COUNT(ACCESS_DTTM) AS AMOUNT\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID, WORKSTATION\n",
    "        ) AS T1\n",
    "        JOIN (SELECT USER_ID, COUNT(ACCESS_DTTM) AS TOTAL\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID\n",
    "        ) AS T2\n",
    "        ON T1.USER_ID = T2.USER_ID\n",
    "    ) AS mainT1 JOIN (\n",
    "        SELECT T.USER_ID, \n",
    "               log10( 1 + (SELECT COUNT(DISTINCT(USER_ID)) AS count FROM {table_name}) * 1.0 / COUNT(T.WORKSTATION) ) AS IDF\n",
    "        FROM (SELECT USER_ID, WORKSTATION\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID, WORKSTATION\n",
    "        ) AS T\n",
    "        GROUP BY T.USER_ID\n",
    "    ) AS mainT2 ON mainT1.USER_ID = mainT2.USER_ID\n",
    "\"\"\".format(table_name=table)\n",
    "\n",
    "\n",
    "for d in range(1, 32):\n",
    "    try: sqlContext_H.dropTempTable(\"df\")\n",
    "    except: pass\n",
    "    try:\n",
    "        df = sqlContext_H.read.parquet(\n",
    "                HOST + \"/parquet2/08/Epic_Access_Log_201608{0:02d}.parquet\".format(d)\n",
    "            ).select(*fields_list).persist()\n",
    "    except:\n",
    "        continue\n",
    "    sqlContext_H.registerDataFrameAsTable(df, 'df')\n",
    "    globals()['df_table_' + str(d)] = sqlContext_H.sql(query(\"df\")).persist()\n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Fit FM \n",
    "svd = TruncatedSVD(n_components=50, n_iter=25, random_state=42)\n",
    "svd.fit(m) \n",
    "print \"svd.explained_variance_ratio_.sum() =\", svd.explained_variance_ratio_.sum()\n",
    "\n",
    "main1 = svd.transform(m)\n",
    "main2 = svd.inverse_transform(main1)\n",
    "\n",
    "main_svd = True\n",
    "\n",
    "# Here we will collect all cosine similarities for both users matrix of factors (cos1) and full FM (cos2)\n",
    "cos1 = {}\n",
    "cos2 = {}\n",
    "\n",
    "for d in range(1, 32):\n",
    "    df_indexedU = indexerU.transform(globals()['df_table_' + str(d)]).withColumn(\n",
    "            \"USER_ID_Index\", F.col(\"USER_ID_Index\").cast(IntegerType())\n",
    "        ).persist()\n",
    "    df_table = indexerW.transform(df_indexedU).withColumn(\n",
    "            \"WORKSTATION_Index\", F.col(\"WORKSTATION_Index\").cast(IntegerType())\n",
    "        ).select(\n",
    "            \"USER_ID_Index\", \"WORKSTATION_Index\", \"TF_IDF\"\n",
    "        ).toPandas()\n",
    "    \n",
    "    df.unpersist()\n",
    "    df_indexedU.unpersist()\n",
    "\n",
    "    df_m = sparse_df_to_array(df_table, (total_users, total_workstations))\n",
    "\n",
    "    if main_svd:\n",
    "        print d\n",
    "        x1 = svd.transform(df_m)\n",
    "        x2 = svd.inverse_transform(x1)\n",
    "    else:\n",
    "        df_svd = TruncatedSVD(n_components=50, n_iter=15, random_state=42)\n",
    "        df_svd.fit(df_m) \n",
    "        print d, \"\\texplained_variance_ratio_ =\", df_svd.explained_variance_ratio_.sum()\n",
    "        x1 = df_svd.transform(df_m)\n",
    "        x2 = df_svd.inverse_transform(x1)\n",
    "    \n",
    "    cos1.update({d:[]})\n",
    "    cos2.update({d:[]})\n",
    "    for i in range(df_m.shape[0]):\n",
    "        cos1[d].append(cosine_similarity(main1[i].reshape(1, -1), x1[i].reshape(1, -1))[0][0])\n",
    "        cos2[d].append(cosine_similarity(main2[i].reshape(1, -1), x2[i].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "renew = False\n",
    "if renew:\n",
    "    svd = TruncatedSVD(n_components=50, n_iter=25, random_state=42)\n",
    "    svd.fit(m) \n",
    "    print \"svd.explained_variance_ratio_.sum() =\", svd.explained_variance_ratio_.sum()\n",
    "    main1 = svd.transform(m)\n",
    "    main2 = svd.inverse_transform(main1)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=25, algorithm='ball_tree', n_jobs=-1).fit(main2)\n",
    "# Array `indices` contains numbers of users with similar behaviour\n",
    "# Array `distances` contains euclidean distances between each pair of users from `indices`\n",
    "distances, indices = nbrs.kneighbors(main2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read users data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "secure_rdd = sc.textFile(HOST + \"/users_info/SecureHealEmpTest.csv\").persist()\n",
    "first = secure_rdd.first()\n",
    "header = first.split(\"|\")\n",
    "row_data = secure_rdd.filter(lambda x: x != first).map( lambda x: x.split(\"|\") ) \\\n",
    "                    .map( lambda p: Row(**{header[i]:p[i] for i in range(len(header))}) ).persist()\n",
    "secure_rdd.unpersist() \n",
    "users = sqlContext.createDataFrame(row_data).select(\n",
    "    \"Employee\", \"JobName\", \"DeptName\", \"ProcDesc\", \"EmpStatus\" \n",
    ").persist()\n",
    "row_data.unpersist();\n",
    "sqlContext_H.registerDataFrameAsTable(users, 'users')\n",
    "\n",
    "users.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Find out relationships between users within each group of 10 elements in `indices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fields_list = ( \"ACCESS_DTTM\", \"USER_ID\", \"WORKSTATION\" )\n",
    "full_data = sqlContext_H.read.parquet(HOST + \"/parquet2/*/*\").select(*fields_list).persist()\n",
    "sqlContext_H.registerDataFrameAsTable(full_data, 'full_data')\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT T.USER_ID, users.JobName, users.DeptName, users.ProcDesc AS HospName, users.EmpStatus,\n",
    "           T.RECORDS_AMOUNT, T.WORKSTATION_AMOUNT, T.WORKSTATIONs\n",
    "    FROM (SELECT USER_ID, COUNT(ACCESS_DTTM) AS RECORDS_AMOUNT, \n",
    "           COUNT(DISTINCT(WORKSTATION)) AS WORKSTATION_AMOUNT, concat_ws('; ', collect_set(WORKSTATION)) AS WORKSTATIONs\n",
    "        FROM full_data\n",
    "        GROUP BY USER_ID\n",
    "    ) AS T\n",
    "    JOIN users\n",
    "    ON T.USER_ID = users.Employee\n",
    "\"\"\"\n",
    "users_grouped = sqlContext_H.sql(query).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_grouped.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Divide nearest neighbours into groups taking into account the amount of users with known job name and department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_known_users = set(map(lambda x: x[0], users.select(\"Employee\").collect()))\n",
    "\n",
    "labels = indexerU.labels\n",
    "indices_users = [set([labels[idx + 1] for idx in lst]) for lst in indices]\n",
    "\n",
    "groups = {i: [] for i in range(0,26)}\n",
    "for num, lst in enumerate(indices_users):\n",
    "    x = len(all_known_users & lst)\n",
    "    flag = True\n",
    "    for _, el in groups[x]:\n",
    "        if set(lst) == set(el):\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        groups[x].append((num, lst))\n",
    "\n",
    "for k, v in groups.iteritems():\n",
    "    print \"Groups of {} known users: {}\".format(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"Distances:\"\n",
    "print distances[groups[25][0][0]]\n",
    "\n",
    "x0 = users_grouped.filter(\"USER_ID IN ({})\".format(\", \".join([\"\\'{}\\'\".format(i) for i in groups[25][0][1]]))).selectExpr(\n",
    "    \"USER_ID\", \"JobName\", \"DeptName\", \"HospName\", \"EmpStatus\", \"RECORDS_AMOUNT as Requests\", \"WORKSTATION_AMOUNT as WS_amount\"\n",
    ").toPandas()\n",
    "x0.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, DataFrame, HiveContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "sqlContext_H = HiveContext(sc)\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fields_list = ( \"ACCESS_DTTM\", \"METRIC_ID\", \"USER_ID\" )\n",
    "data = sqlContext_H.read.parquet( HOST + \"/parquet2/07/*\" ).select(*fields_list).persist()\n",
    "sqlContext_H.registerDataFrameAsTable(data, 'data') \n",
    "\n",
    "# 1. Count all unique users of the defined month\n",
    "total_users = sc.textFile(HOST + \"/csv/unique_users_7.csv\").count()\n",
    "\n",
    "# 2. Calculate tf\n",
    "query = lambda total_users: \"\"\"\n",
    "    SELECT T1.USER_ID, T1.METRIC_ID, T1.AMOUNT*1.0 / T2.TOTAL AS TF \n",
    "    FROM (\n",
    "        SELECT USER_ID, METRIC_ID, COUNT(ACCESS_DTTM) AS AMOUNT\n",
    "        FROM data\n",
    "        GROUP BY USER_ID, METRIC_ID\n",
    "    ) AS T1\n",
    "    JOIN (\n",
    "        SELECT USER_ID, COUNT(ACCESS_DTTM) AS TOTAL\n",
    "        FROM data\n",
    "        GROUP BY USER_ID\n",
    "    ) AS T2\n",
    "    ON T1.USER_ID = T2.USER_ID\n",
    "\"\"\".format(total_users=total_users)\n",
    "\n",
    "res = sqlContext_H.sql(query(total_users)).persist()\n",
    "# 3. Encode a string column of labels to a column of label indices\n",
    "all_users = sqlContext_H.createDataFrame( \n",
    "        sc.textFile(HOST + \"/csv/unique_users.csv\").map(lambda p: Row(USER_ID=p)) \n",
    "    ).union( sqlContext_H.createDataFrame([Row(USER_ID='')]) ).persist()\n",
    "print \"all_users =\", all_users.count()\n",
    "\n",
    "try:\n",
    "    all_metrics = sqlContext_H.createDataFrame( \n",
    "            sc.textFile(HOST + \"/csv/unique_metrics.csv\").map(lambda p: Row(METRIC_ID=p)) \n",
    "        ).union( sqlContext_H.createDataFrame([Row(METRIC_ID='')]) ).persist()\n",
    "except:\n",
    "    all_metrics = sqlContext_H.read.parquet( \n",
    "            HOST + \"/parquet2/07/*\",\n",
    "            HOST + \"/parquet2/08/*\",\n",
    "            HOST + \"/parquet2/09/*\"\n",
    "        ).select(\"METRIC_ID\").distinct().persist()\n",
    "    all_metrics.write.format(\"com.databricks.spark.csv\").mode('overwrite').save(HOST + \"/csv/unique_metrics.csv\")\n",
    "    all_metrics = all_metrics.union( sqlContext_H.createDataFrame([Row(METRIC_ID='')]) )\n",
    "print \"METRIC_IDs = \", all_metrics.count()\n",
    "\n",
    "indexerU = StringIndexer(inputCol=\"USER_ID\", outputCol=\"USER_ID_Index\").fit(all_users)\n",
    "indexerM = StringIndexer(inputCol=\"METRIC_ID\", outputCol=\"METRIC_ID_Index\").fit(all_metrics)\n",
    "\n",
    "all_users.unpersist()\n",
    "all_metrics.unpersist()\n",
    "\n",
    "indexedU_df = indexerU.transform(res).withColumn(\n",
    "        \"USER_ID_Index\", F.col(\"USER_ID_Index\").cast(IntegerType())\n",
    "    ).persist()\n",
    "\n",
    "table = indexerM.transform(indexedU_df).withColumn(\n",
    "        \"METRIC_ID_Index\", F.col(\"METRIC_ID_Index\").cast(IntegerType())\n",
    "    ).persist()\n",
    "\n",
    "res.unpersist()\n",
    "indexedU_df.unpersist()\n",
    "sqlContext_H.dropTempTable('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "t = table.select(\"USER_ID_Index\", \"METRIC_ID_Index\", \"TF\").toPandas()\n",
    "\n",
    "total_users = sc.textFile(HOST + \"/csv/unique_users.csv\").count()\n",
    "total_metrics = sc.textFile(HOST + \"/csv/unique_metrics.csv\").count()\n",
    "\n",
    "def sparse_df_to_array(df, shape):\n",
    "    \"\"\" Convert sparse dataframe to sparse array csr_matrix used by scikit learn. \"\"\"\n",
    "    arr = lil_matrix(shape, dtype=np.float32)\n",
    "    for i in range(df.shape[0]):\n",
    "        arr[df.ix[i, \"USER_ID_Index\"]-1, df.ix[i, \"METRIC_ID_Index\"]-1] = df.ix[i, \"TF\"]\n",
    "    return arr.tocsr()\n",
    "\n",
    "m = sparse_df_to_array(t, (total_users, total_metrics))\n",
    "\n",
    "svd = TruncatedSVD(n_components=35, n_iter=25, random_state=42)\n",
    "svd.fit(m)\n",
    "print \"svd.explained_variance_ratio_.sum() =\", svd.explained_variance_ratio_.sum()\n",
    "\n",
    "#factors = svd.transform(m)\n",
    "#remanufactured = svd.inverse_transform(factors)\n",
    "\n",
    "\"\"\"m_idx = np.where(m.todense() > 0)\n",
    "m_not_0 = m.todense()[m_idx].T\n",
    "remanufactured_not_0 = remanufactured[m_idx].reshape(m_not_0.shape[0], 1)\n",
    "print \"RMSE:\", np.sqrt(np.square(m_not_0 - remanufactured_not_0).mean())\"\"\";\n",
    "\n",
    "table.unpersist()\n",
    "del m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Below 0 corresponds to the 7th month \n",
    "week_talbes = {0: t}\n",
    "week_unique_metrics = {0: t[\"METRIC_ID_Index\"].unique()}\n",
    "del t\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT T1.USER_ID, T1.METRIC_ID, T1.AMOUNT*1.0 / T2.TOTAL AS TF \n",
    "    FROM (\n",
    "        SELECT USER_ID, METRIC_ID, COUNT(ACCESS_DTTM) AS AMOUNT\n",
    "        FROM df\n",
    "        GROUP BY USER_ID, METRIC_ID\n",
    "    ) AS T1\n",
    "    JOIN (\n",
    "        SELECT USER_ID, COUNT(ACCESS_DTTM) AS TOTAL\n",
    "        FROM df\n",
    "        GROUP BY USER_ID\n",
    "    ) AS T2\n",
    "    ON T1.USER_ID = T2.USER_ID\n",
    "\"\"\"\n",
    "\n",
    "data_8_9 = sqlContext_H.read.parquet( \n",
    "        HOST + \"/parquet2/08/*\",\n",
    "        HOST + \"/parquet2/09/*\"\n",
    "    ).persist()\n",
    "sqlContext_H.registerDataFrameAsTable(data_8_9, 'data_8_9')\n",
    "\n",
    "#week_factors = {}\n",
    "week_remanufactured = {}\n",
    "week_matrices = {}\n",
    "\n",
    "for w in range(31, 39):\n",
    "    try: sqlContext_H.dropTempTable(\"df\")\n",
    "    except: pass\n",
    "    print w\n",
    "    df = data_8_9.filter(\"ACCESS_WEEK = {}\".format(w)).select(*fields_list).persist()\n",
    "    sqlContext_H.registerDataFrameAsTable(df, 'df')\n",
    "    df_w = sqlContext_H.sql(query).persist()\n",
    "    df.unpersist()\n",
    "    df_indexedU = indexerU.transform(df_w).withColumn(\n",
    "            \"USER_ID_Index\", F.col(\"USER_ID_Index\").cast(IntegerType())\n",
    "        ).persist()\n",
    "    df_table = indexerM.transform(df_indexedU).withColumn(\n",
    "            \"METRIC_ID_Index\", F.col(\"METRIC_ID_Index\").cast(IntegerType())\n",
    "        ).select( \"USER_ID_Index\", \"METRIC_ID_Index\", \"TF\" ).toPandas()\n",
    "    df_w.unpersist()\n",
    "    df_indexedU.unpersist()\n",
    "    \n",
    "    week_talbes[w] = df_table\n",
    "    week_unique_metrics[w] = df_table[\"METRIC_ID_Index\"].unique()\n",
    "    \n",
    "    df_m = sparse_df_to_array(df_table, (total_users, total_metrics))\n",
    "    del df_table\n",
    "    week_matrices[w] = df_m.todense()\n",
    "    #week_factors[w] = svd.transform(df_m)\n",
    "    week_factors = svd.transform(df_m)\n",
    "    week_remanufactured[w] = svd.inverse_transform(week_factors)\n",
    "    del df_m\n",
    "    del week_factors\n",
    "    \n",
    "\n",
    "data_8_9.unpersist()\n",
    "sqlContext_H.dropTempTable('data_8_9')\n",
    "del total_users\n",
    "del total_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Remove unused metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:\n",
    "    residuals = {}\n",
    "    init_shape = week_remanufactured[31].shape\n",
    "    print \"Initial shape = {}\".format(init_shape)\n",
    "    diff = {}\n",
    "    for w in range(31, 39):\n",
    "        indices = np.array(list(set(week_unique_metrics[w]) | set(week_unique_metrics[0]))) - 1\n",
    "        #residuals[w] = np.take(week_remanufactured[w], indices) - np.take(week_matrices[w], indices)\n",
    "        residuals[w] = week_remanufactured[w][:, indices] - week_matrices[w][:, indices]\n",
    "        print \"Week = {} (shape = {}): AVG = {}, STD = {}\".format(w, residuals[w].shape, np.average(residuals[w]), np.std(residuals[w]))\n",
    "        np.savetxt(\"fm7/metrics_residuals_{}_common_values.txt\".format(w), residuals[w], delimiter=\",\")\n",
    "        diff[w] = (init_shape[1] - indices.size) * init_shape[0]\n",
    "    avg_diff = np.average(diff.values()) \n",
    "    print \"Difference: {0} ({1:.02f}%)\".format(avg_diff, avg_diff / float(init_shape[0] * init_shape[1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "residuals = {}\n",
    "for w in range(31, 39):\n",
    "    residuals[w] = np.loadtxt(\"fm7/metrics_residuals_{}_common_values.txt\".format(w), delimiter=\",\")\n",
    "    print \"Week = {}: AVG = {}, STD = {}\".format(w, np.average(residuals[w]), np.std(residuals[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "b = 1000\n",
    "#z = np.absolute(residuals[31].reshape(-1,))\n",
    "z = residuals[31].reshape(-1,)\n",
    "#z = z[(abs(z) > 0.05) & (abs(z) < 0.5)]\n",
    "print \"mean =\", np.average(z), \"variance =\", np.std(z)\n",
    "pdf, bins, patches = plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "\n",
    "x, dx = np.linspace(-1.5, 1.5, b, retstep=True)\n",
    "y = mlab.normpdf(x, np.average(z), np.std(z))\n",
    "\n",
    "print \"Area formula:\", np.trapz(y, dx=dx)\n",
    "print \"Area real:\", np.sum(pdf * np.diff(bins))\n",
    "\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "plt.plot(x, y, 'r-')\n",
    "\n",
    "axes.set_ylim([0,0.5])\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,0.01])\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Remove unused metrics and non active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:\n",
    "    residuals = {}\n",
    "    init_shape = week_remanufactured[31].shape\n",
    "    print \"Initial shape = {}\".format(init_shape)\n",
    "    diff = {}\n",
    "    for w in range(31, 39):\n",
    "        indices = np.array(list(set(week_unique_metrics[w]) | set(week_unique_metrics[0]))) - 1\n",
    "        rows_0, _ = np.where( (week_matrices[w] == 0).all(axis=1) )\n",
    "        rows = np.array(list(set(range(init_shape[0])) - set(rows_0)))\n",
    "        residuals[w] = week_remanufactured[w][:, indices][rows, :] - week_matrices[w][:, indices][rows, :]\n",
    "        print \"Week = {} (shape = {}): AVG = {}, STD = {}\".format(w, residuals[w].shape, np.average(residuals[w]), np.std(residuals[w]))\n",
    "        np.savetxt(\"fm7/metrics_residuals_{}_common_values_without_non_active_users.txt\".format(w), residuals[w], delimiter=\",\")\n",
    "        diff[w] = init_shape[1] * init_shape[0] - indices.size * rows.size\n",
    "    avg_diff = np.average(diff.values()) \n",
    "    print \"Difference: {0} ({1:.02f}%)\".format(avg_diff, avg_diff / float(init_shape[0] * init_shape[1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "residuals = {}\n",
    "for w in range(31, 39):\n",
    "    residuals[w] = np.loadtxt(\"fm7/metrics_residuals_{}_common_values_without_non_active_users.txt\".format(w), delimiter=\",\")\n",
    "    print \"Week = {}: AVG = {}, STD = {}\".format(w, np.average(residuals[w]), np.std(residuals[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 12866 * 689\n",
    "print 9710 * 668\n",
    "print residuals[31].reshape(-1,).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "b = 1000\n",
    "#z = np.absolute(residuals[31].reshape(-1,))\n",
    "z = residuals[31].reshape(-1,)\n",
    "#z = z[(abs(z) > 0.05) & (abs(z) < 0.5)]\n",
    "print \"mean =\", np.average(z), \"variance =\", np.std(z)\n",
    "pdf, bins, patches = plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "\n",
    "x, dx = np.linspace(-1.5, 1.5, b, retstep=True)\n",
    "y = mlab.normpdf(x, np.average(z), np.std(z))\n",
    "\n",
    "print \"Area formula:\", np.trapz(y, dx=dx)\n",
    "print \"Area real:\", np.sum(pdf * np.diff(bins))\n",
    "\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "plt.plot(x, y, 'r-')\n",
    "\n",
    "axes.set_ylim([0,0.5])\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,0.01])\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Remove unused metrics + users pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0 = week_talbes[0].groupby(\"USER_ID_Index\")['METRIC_ID_Index'].apply(lambda x: \"%s\" % ', '.join(map(lambda y: str(y), x))).to_frame()\n",
    "w0.reset_index(level=0, inplace=True)\n",
    "w31 = week_talbes[31].groupby(\"USER_ID_Index\")['METRIC_ID_Index'].apply(lambda x: \"%s\" % ', '.join(map(lambda y: str(y), x))).to_frame()\n",
    "w31.reset_index(level=0, inplace=True)\n",
    "w = w0.join(w31, on='USER_ID_Index', how='outer', lsuffix='_left', rsuffix='_right')\n",
    "w[\"metrics\"] = (w[\"METRIC_ID_Index_left\"] + \", \" + w[\"METRIC_ID_Index_right\"]).apply(\n",
    "        lambda x: list(set(map(lambda y: int(y.strip()) if y != 'nan' else -1, str(x).split(\",\"))))\n",
    "    )\n",
    "w.drop([\"USER_ID_Index_left\", \"USER_ID_Index_right\", \"METRIC_ID_Index_left\", \"METRIC_ID_Index_right\"], inplace=True, axis=1)\n",
    "w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "residuals = {}\n",
    "init_shape = week_remanufactured[31].shape\n",
    "print \"Initial shape = {}\".format(init_shape)\n",
    "\n",
    "metrics_7 = week_talbes[0].groupby(\"USER_ID_Index\")['METRIC_ID_Index'].apply(lambda x: \"%s\" % ', '.join(map(lambda y: str(y), x))).to_frame()\n",
    "metrics_7.reset_index(level=0, inplace=True)\n",
    "diff = {}\n",
    "for w in range(31, 39):\n",
    "    metrics_w = week_talbes[w].groupby(\"USER_ID_Index\")['METRIC_ID_Index'].apply(lambda x: \"%s\" % ', '.join(map(lambda y: str(y), x))).to_frame()\n",
    "    metrics_w.reset_index(level=0, inplace=True)\n",
    "    joined = w0.join(metrics_w, on='USER_ID_Index', how='outer', lsuffix='_left', rsuffix='_right')\n",
    "    joined[\"metrics\"] = (joined[\"METRIC_ID_Index_left\"] + \", \" + joined[\"METRIC_ID_Index_right\"]).apply(\n",
    "            lambda x: list(set(map(lambda y: int(y.strip()) if y != 'nan' else -1, str(x).split(\",\"))))\n",
    "        )\n",
    "    joined.drop([\"USER_ID_Index_left\", \"USER_ID_Index_right\", \"METRIC_ID_Index_left\", \"METRIC_ID_Index_right\"], inplace=True, axis=1)\n",
    "    residuals[w] = np.array([[]])\n",
    "    for i, vals in joined.iterrows():\n",
    "        ind = [x-1 for x in vals[\"metrics\"] if x>0]\n",
    "        u = vals[\"USER_ID_Index\"]-1\n",
    "        residuals[w] = np.append(residuals[w], week_remanufactured[w][u, ind] - week_matrices[w][u, ind])\n",
    "    print \"Week = {} (shape = {}): AVG = {}, STD = {}\".format(w, residuals[w].shape, np.average(residuals[w]), np.std(residuals[w]))\n",
    "    np.savetxt(\"fm7/metrics_residuals_{}_only_present_metric_user_pairs.txt\".format(w), residuals[w], delimiter=\",\")\n",
    "    diff[w] = init_shape[0] * init_shape[1] - residuals[w].size\n",
    "    del metrics_w\n",
    "    del joined\n",
    "avg_diff = np.average(diff.values()) \n",
    "print \"Difference: {0} ({1:.02f}%)\".format(avg_diff, avg_diff / float(init_shape[0] * init_shape[1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "b = 1000\n",
    "#z = np.absolute(residuals[31].reshape(-1,))\n",
    "z = residuals[31].reshape(-1,)\n",
    "#z = z[(abs(z) > 0.05) & (abs(z) < 0.5)]\n",
    "print \"mean =\", np.average(z), \"variance =\", np.std(z)\n",
    "pdf, bins, patches = plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "\n",
    "x, dx = np.linspace(-1.5, 1.5, b, retstep=True)\n",
    "y = mlab.normpdf(x, np.average(z), np.std(z))\n",
    "\n",
    "print \"Area formula:\", np.trapz(y, dx=dx)\n",
    "print \"Area real:\", np.sum(pdf * np.diff(bins))\n",
    "\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,0.5])\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(z, bins=b, alpha=0.5, normed=1)\n",
    "plt.plot(x, y, 'r-')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,0.01])\n",
    "axes.set_xlim([-1.1,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "for i,j in enumerate(range(31, 39)):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    pdf, bins, patches = plt.hist(residuals[j].reshape(-1,), bins=100, alpha=0.75, normed=1)\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,0.2])\n",
    "    plt.title(\"week = {0}, area = {1:0.3f}\".format(j, np.sum(pdf * np.diff(bins))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "for i,j in enumerate(range(31, 39)):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.hist(residuals[j].reshape(-1,), bins=250, alpha=0.75)\n",
    "    plt.title(j)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "colors = ('r', 'g', 'b', 'y', 'c', 'm', 'grey', 'pink')\n",
    "for i,j in enumerate(range(31, 39)):\n",
    "    plt.hist(residuals[j].reshape(-1,), bins=100, alpha=0.4, color=colors[i])\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,2500])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
